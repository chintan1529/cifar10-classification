{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SuTMYi-xNvA",
        "outputId": "36dc11b8-1833-48f2-f7a1-7b6ec761749c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Setting up\n",
        "import os, random, math, time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: CIFAR-10 data and train/val split\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# CIFAR 10 classes\n",
        "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# Normalization\n",
        "mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# Baseline transformerr\n",
        "tfm_base_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "tfm_base_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# Load CIFAR 10\n",
        "root = \"./data\"\n",
        "train_full = datasets.CIFAR10(root=root, train=True, download=True, transform=tfm_base_train)\n",
        "test_set   = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm_base_test)\n",
        "\n",
        "print(f\"Full train: {len(train_full)}, Test: {len(test_set)}\")\n",
        "\n",
        "# Stratified split\n",
        "def stratified_split(dataset, val_frac=0.1, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    buckets = defaultdict(list)\n",
        "    for idx in range(len(dataset)):\n",
        "        _, y = dataset[idx]\n",
        "        buckets[int(y)].append(idx)\n",
        "\n",
        "    train_idx, val_idx = [], []\n",
        "    for y, idxs in buckets.items():\n",
        "        idxs = np.array(idxs)\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = int(len(idxs) * val_frac)\n",
        "        val_idx.extend(idxs[:n_val].tolist())\n",
        "        train_idx.extend(idxs[n_val:].tolist())\n",
        "\n",
        "    return train_idx, val_idx\n",
        "\n",
        "train_idx, val_idx = stratified_split(train_full)\n",
        "print(f\"Train indices: {len(train_idx)}, Val indices: {len(val_idx)}\")\n",
        "print(\"Data is ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NXDKd1exqxd",
        "outputId": "8a0356cf-4f5b-41c1-e696-1ca689dc6d88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:06<00:00, 25.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full train: 50000, Test: 10000\n",
            "Train indices: 45000, Val indices: 5000\n",
            "âœ… Data ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloaders\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    import random\n",
        "    import numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "# Creating dataloaders\n",
        "def make_loaders(train_full, test_set, train_idx, val_idx, batch_size=128):\n",
        "    train_set = Subset(train_full, train_idx)\n",
        "    val_set   = Subset(train_full, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
        "                             num_workers=2, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                             num_workers=2, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Baseline dataloaders\n",
        "train_loader, val_loader, test_loader = make_loaders(train_full, test_set, train_idx, val_idx)\n",
        "\n",
        "print(f\"Dataloaders are ready\")\n",
        "print(f\"Train batches: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")\n",
        "print(f\"Batch shape:\")\n",
        "x, y = next(iter(train_loader))\n",
        "print(f\"First batch - x: {x.shape}, y: {y.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTR-4iPDx84Z",
        "outputId": "6c6ea862-ad62-4078-868b-35d9fb9e6f72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "âœ… Dataloaders ready!\n",
            "Train batches: 352, Val: 40, Test: 79\n",
            "Batch shape: let's check first batch\n",
            "First batch - x: torch.Size([128, 3, 32, 32]), y: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Simple CNN - 3 conv blocks\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  # 32x32 -> 16x16\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  # 16x16 -> 8x8\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  # 8x8 -> 4x4\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 4, 256), nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "# Training util\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    all_y, all_p = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        p = logits.argmax(1)\n",
        "        total += y.size(0)\n",
        "        correct += (p == y).sum().item()\n",
        "        all_y.append(y.cpu().numpy())\n",
        "        all_p.append(p.cpu().numpy())\n",
        "    acc = correct / total\n",
        "    y_true = np.concatenate(all_y)\n",
        "    y_pred = np.concatenate(all_p)\n",
        "    return acc, y_true, y_pred\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total, correct = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        total += y.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "print(\" Baseline model and utilities ready\")\n",
        "print(f\"Model params: {sum(p.numel() for p in SmallCNN().parameters()):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLFZZotRyEXp",
        "outputId": "d7c3b733-de4e-4332-d870-ff91ef051a77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Baseline model + utilities ready!\n",
            "Model params: 666,538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Baseline SmallCNN - 15 epochs\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Initialization\n",
        "baseline = SmallCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(baseline.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "# Track best validation accuracy\n",
        "best_val_acc = 0.0\n",
        "best_state = None\n",
        "\n",
        "print(\"Starting baseline training...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for epoch in range(1, 16):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_one_epoch(baseline, train_loader, optimizer, criterion)\n",
        "\n",
        "    # Validate\n",
        "    val_acc, _, _ = evaluate(baseline, val_loader)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_state = {k: v.cpu().clone() for k, v in baseline.state_dict().items()}\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | Loss: {train_loss:.4f} | Train: {train_acc:.3f} | Val: {val_acc:.3f}\")\n",
        "\n",
        "# Load best model\n",
        "baseline.load_state_dict(best_state)\n",
        "print(\"\\n Baseline training complete\")\n",
        "print(f\"Best val accuracy: {best_val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOY9L74Vym5K",
        "outputId": "afcf03bc-2bb3-4ead-9edf-dd5d4c7757d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting baseline training...\n",
            "------------------------------------------------------------\n",
            "Epoch  1 | Loss: 1.5260 | Train: 0.436 | Val: 0.576\n",
            "Epoch  2 | Loss: 1.0917 | Train: 0.609 | Val: 0.669\n",
            "Epoch  3 | Loss: 0.8691 | Train: 0.696 | Val: 0.716\n",
            "Epoch  4 | Loss: 0.7354 | Train: 0.745 | Val: 0.760\n",
            "Epoch  5 | Loss: 0.6318 | Train: 0.778 | Val: 0.777\n",
            "Epoch  6 | Loss: 0.5516 | Train: 0.806 | Val: 0.782\n",
            "Epoch  7 | Loss: 0.4805 | Train: 0.830 | Val: 0.785\n",
            "Epoch  8 | Loss: 0.4151 | Train: 0.852 | Val: 0.790\n",
            "Epoch  9 | Loss: 0.3619 | Train: 0.871 | Val: 0.793\n",
            "Epoch 10 | Loss: 0.3064 | Train: 0.891 | Val: 0.789\n",
            "Epoch 11 | Loss: 0.2728 | Train: 0.901 | Val: 0.797\n",
            "Epoch 12 | Loss: 0.2314 | Train: 0.917 | Val: 0.794\n",
            "Epoch 13 | Loss: 0.2129 | Train: 0.923 | Val: 0.789\n",
            "Epoch 14 | Loss: 0.1863 | Train: 0.934 | Val: 0.801\n",
            "Epoch 15 | Loss: 0.1798 | Train: 0.936 | Val: 0.788\n",
            "\n",
            "âœ… Baseline training complete!\n",
            "Best val accuracy: 0.801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline test evaluation + confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Test evaluation\n",
        "test_acc, y_true, y_pred = evaluate(baseline, test_loader)\n",
        "print(f\" Baseline TEST accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
        "\n",
        "# Save baseline results\n",
        "baseline_results = {\n",
        "    \"test_accuracy\": float(test_acc),\n",
        "    \"confusion_matrix\": cm.tolist()\n",
        "}\n",
        "print(f\"\\n Baseline complete Test acc: {test_acc:.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vt-jhBtP0AM2",
        "outputId": "c3d37a94-106f-49aa-d33d-eec6adccefd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Baseline TEST accuracy: 0.7873 (78.7%)\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[822  10  48  13  18   1   8   7  42  31]\n",
            " [ 16 884   1   4   3   1   7   0  14  70]\n",
            " [ 54   5 695  44  70  53  55  10   4  10]\n",
            " [ 22   4  55 605  73 123  71  18   9  20]\n",
            " [ 10   2  41  41 813  20  44  19   4   6]\n",
            " [  9   2  32 155  42 692  25  33   2   8]\n",
            " [  2   3  32  39  32  18 862   3   6   3]\n",
            " [ 16   3  35  48  76  51   7 753   1  10]\n",
            " [ 56  20   8   7   8   5   7   4 857  28]\n",
            " [ 28  49   2   8   1   2   6   5   9 890]]\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane     0.7942    0.8220    0.8079      1000\n",
            "  automobile     0.9002    0.8840    0.8920      1000\n",
            "        bird     0.7323    0.6950    0.7132      1000\n",
            "         cat     0.6276    0.6050    0.6161      1000\n",
            "        deer     0.7157    0.8130    0.7612      1000\n",
            "         dog     0.7164    0.6920    0.7040      1000\n",
            "        frog     0.7894    0.8620    0.8241      1000\n",
            "       horse     0.8838    0.7530    0.8132      1000\n",
            "        ship     0.9040    0.8570    0.8799      1000\n",
            "       truck     0.8271    0.8900    0.8574      1000\n",
            "\n",
            "    accuracy                         0.7873     10000\n",
            "   macro avg     0.7891    0.7873    0.7869     10000\n",
            "weighted avg     0.7891    0.7873    0.7869     10000\n",
            "\n",
            "\n",
            "âœ… Baseline complete! Test acc: 78.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet18 transfer learning and augmentation\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "print(\"Setting up ResNet18 (ImageNet pretrained)...\")\n",
        "\n",
        "# ResNet transforms\n",
        "mean_imagenet = (0.485, 0.456, 0.406)\n",
        "std_imagenet  = (0.229, 0.224, 0.225)\n",
        "\n",
        "\n",
        "tfm_strong_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_imagenet, std_imagenet),\n",
        "])\n",
        "\n",
        "# Test transforms without augmentation\n",
        "tfm_strong_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_imagenet, std_imagenet),\n",
        "])\n",
        "\n",
        "# Reload datasets with the new transforms\n",
        "train_full_strong = datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=tfm_strong_train)\n",
        "test_set_strong   = datasets.CIFAR10(root=\"./data\", train=False, download=False, transform=tfm_strong_test)\n",
        "\n",
        "# Same stratified split indices work\n",
        "strong_train_loader, strong_val_loader, strong_test_loader = make_loaders(\n",
        "    train_full_strong, test_set_strong, train_idx, val_idx, batch_size=64\n",
        ")\n",
        "\n",
        "print(f\"ResNet dataloaders ready Batch size: 64\")\n",
        "print(f\"Train batches: {len(strong_train_loader)}, Val: {len(strong_val_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y99ZUIpQ0jpu",
        "outputId": "a2e1584a-aec5-4f2e-efb4-239ff9a9880f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ResNet18 (ImageNet pretrained)...\n",
            "ResNet dataloaders ready Batch size: 64\n",
            "Train batches: 704, Val: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train ResNet18 (frozen backbone --- full fine-tune)\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pretrained ResNet18, replace classifier\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)  # CIFAR-10 classes\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "print(f\"ResNet18 total params: {sum(p.numel() for p in resnet18.parameters()):,}\")\n",
        "print(\"Phase 1: Freeze backbone, train classifier only\")\n",
        "\n",
        "# Phase 1: Freeze backbone, train classifier\n",
        "for name, param in resnet18.named_parameters():\n",
        "    if not name.startswith('fc.'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW([p for p in resnet18.parameters() if p.requires_grad],\n",
        "                       lr=3e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 9):\n",
        "    train_loss, train_acc = train_one_epoch(resnet18, strong_train_loader, optimizer, criterion)\n",
        "    val_acc, _, _ = evaluate(resnet18, strong_val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_state = {k: v.cpu().clone() for k, v in resnet18.state_dict().items()}\n",
        "\n",
        "    print(f\"Frozen [{epoch:2d}/8] | Loss: {train_loss:.3f} | Train: {train_acc:.3f} | Val: {val_acc:.3f}\")\n",
        "\n",
        "resnet18.load_state_dict(best_state)\n",
        "print(f\"\\n Phase 1 complete! Best val: {best_val_acc:.1%}\")\n",
        "\n",
        "print(\"\\nPhase 2: Unfreeze all, fine-tune (lr=1e-4)...\")\n",
        "\n",
        "# Phase 2: Unfreeze everything, lower LR\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = optim.AdamW(resnet18.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=12)\n",
        "\n",
        "for epoch in range(1, 13):\n",
        "    train_loss, train_acc = train_one_epoch(resnet18, strong_train_loader, optimizer, criterion)\n",
        "    val_acc, _, _ = evaluate(resnet18, strong_val_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_state = {k: v.cpu().clone() for k, v in resnet18.state_dict().items()}\n",
        "\n",
        "    print(f\"Fine-tune [{epoch:2d}/12] | Loss: {train_loss:.3f} | Train: {train_acc:.3f} | Val: {val_acc:.3f}\")\n",
        "\n",
        "resnet18.load_state_dict(best_state)\n",
        "print(f\"\\n ResNet18 training COMPLETE! Final best val: {best_val_acc:.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FDc3-2E1ASf",
        "outputId": "72a893e2-7bf5-4515-af05-77465b854277"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 86.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet18 total params: 11,181,642\n",
            "Phase 1: Freeze backbone, train classifier only...\n",
            "Frozen [ 1/8] | Loss: 1.194 | Train: 0.713 | Val: 0.744\n",
            "Frozen [ 2/8] | Loss: 1.119 | Train: 0.750 | Val: 0.743\n",
            "Frozen [ 3/8] | Loss: 1.106 | Train: 0.757 | Val: 0.764\n",
            "Frozen [ 4/8] | Loss: 1.086 | Train: 0.765 | Val: 0.777\n",
            "Frozen [ 5/8] | Loss: 1.072 | Train: 0.771 | Val: 0.790\n",
            "Frozen [ 6/8] | Loss: 1.055 | Train: 0.781 | Val: 0.774\n",
            "Frozen [ 7/8] | Loss: 1.040 | Train: 0.786 | Val: 0.785\n",
            "Frozen [ 8/8] | Loss: 1.032 | Train: 0.789 | Val: 0.785\n",
            "\n",
            "âœ… Phase 1 complete! Best val: 79.0%\n",
            "\n",
            "Phase 2: Unfreeze all, fine-tune (lr=1e-4)...\n",
            "Fine-tune [ 1/12] | Loss: 0.809 | Train: 0.893 | Val: 0.933\n",
            "Fine-tune [ 2/12] | Loss: 0.679 | Train: 0.947 | Val: 0.938\n",
            "Fine-tune [ 3/12] | Loss: 0.629 | Train: 0.967 | Val: 0.953\n",
            "Fine-tune [ 4/12] | Loss: 0.597 | Train: 0.979 | Val: 0.955\n",
            "Fine-tune [ 5/12] | Loss: 0.578 | Train: 0.985 | Val: 0.956\n",
            "Fine-tune [ 6/12] | Loss: 0.559 | Train: 0.992 | Val: 0.959\n",
            "Fine-tune [ 7/12] | Loss: 0.546 | Train: 0.995 | Val: 0.961\n",
            "Fine-tune [ 8/12] | Loss: 0.537 | Train: 0.997 | Val: 0.963\n",
            "Fine-tune [ 9/12] | Loss: 0.531 | Train: 0.998 | Val: 0.965\n",
            "Fine-tune [10/12] | Loss: 0.526 | Train: 0.999 | Val: 0.963\n",
            "Fine-tune [11/12] | Loss: 0.523 | Train: 0.999 | Val: 0.968\n",
            "Fine-tune [12/12] | Loss: 0.522 | Train: 1.000 | Val: 0.966\n",
            "\n",
            "ðŸš€ ResNet18 training COMPLETE! Final best val: 96.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final test and  comparison\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Test ResNet18\n",
        "resnet_test_acc, y_true_res, y_pred_res = evaluate(resnet18, strong_test_loader)\n",
        "print(f\"ResNet18 TEST accuracy: {resnet_test_acc:.4f} ({resnet_test_acc*100:.1f}%)\")\n",
        "\n",
        "# ResNet confusion matrix and report\n",
        "cm_res = confusion_matrix(y_true_res, y_pred_res)\n",
        "print(\"\\nResNet Confusion Matrix (first 5 rows):\")\n",
        "print(cm_res[:5])\n",
        "print(\"\\nResNet Classification Report (top 5):\")\n",
        "report_res = classification_report(y_true_res, y_pred_res, target_names=classes, digits=4, output_dict=True)\n",
        "for cls in classes[:5]:\n",
        "    print(f\"{cls}: {report_res[cls]['f1-score']:.4f}\")\n",
        "\n",
        "# Comparing\n",
        "baseline_test_acc = 0.7873\n",
        "baseline_val_acc = 0.801\n",
        "resnet_val_acc = 0.968\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline CNN\", \"ResNet18 TL\"],\n",
        "    \"Test Acc\": [f\"{baseline_test_acc:.1%}\", f\"{resnet_test_acc:.1%}\"],\n",
        "    \"Val Acc\": [f\"{baseline_val_acc:.1%}\", f\"{resnet_val_acc:.1%}\"],\n",
        "    \"Improvement\": [\"-\", f\"+{((resnet_test_acc/baseline_test_acc-1)*100):.0f}%\"]\n",
        "})\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(comparison.to_markdown(index=False))\n",
        "\n",
        "# Saving results JSON for readme\n",
        "final_results = {\n",
        "    \"baseline_test_acc\": float(baseline_test_acc),\n",
        "    \"resnet_test_acc\": float(resnet_test_acc),\n",
        "    \"classes\": classes\n",
        "}\n",
        "print(\"\\nResults saved! Copy final_results to metrics.json\")\n",
        "print(json.dumps(final_results, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGB_ILofLDyM",
        "outputId": "d9619674-baae-4e52-c670-ca9e4909f939"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet18 TEST accuracy: 0.9640 (96.4%)\n",
            "\n",
            "ResNet Confusion Matrix (first 5 rows):\n",
            "[[973   0   5   1   0   1   1   2  13   4]\n",
            " [  1 982   0   1   0   0   0   0   3  13]\n",
            " [  6   0 961   8   9   7   5   3   1   0]\n",
            " [  3   0   9 922  14  40   5   2   4   1]\n",
            " [  1   0   4   9 976   1   2   7   0   0]]\n",
            "\n",
            "ResNet Classification Report (top 5):\n",
            "airplane: 0.9725\n",
            "automobile: 0.9796\n",
            "bird: 0.9639\n",
            "cat: 0.9160\n",
            "deer: 0.9654\n",
            "\n",
            "Model Comparison:\n",
            "| Model        | Test Acc   | Val Acc   | Improvement   |\n",
            "|:-------------|:-----------|:----------|:--------------|\n",
            "| Baseline CNN | 78.7%      | 80.1%     | -             |\n",
            "| ResNet18 TL  | 96.4%      | 96.8%     | +22%          |\n",
            "\n",
            "Results saved! Copy final_results to metrics.json\n",
            "{\n",
            "  \"baseline_test_acc\": 0.7873,\n",
            "  \"resnet_test_acc\": 0.964,\n",
            "  \"classes\": [\n",
            "    \"airplane\",\n",
            "    \"automobile\",\n",
            "    \"bird\",\n",
            "    \"cat\",\n",
            "    \"deer\",\n",
            "    \"dog\",\n",
            "    \"frog\",\n",
            "    \"horse\",\n",
            "    \"ship\",\n",
            "    \"truck\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}